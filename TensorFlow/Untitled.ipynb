{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "# setup\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from pprint import pprint\n",
    "#%matplotlib inline\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soynlp로 텍스트 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2016-10-20.txt', <http.client.HTTPMessage at 0x22a036c5e50>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "from soynlp import DoublespaceLineCorpus\n",
    "from soynlp.word import WordExtractor\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/lovit/soynlp/master/tutorials/2016-10-20.txt\", filename=\"2016-10-20.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30091"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련 데이터를 다수의 문서로 분리\n",
    "corpus = DoublespaceLineCorpus(\"2016-10-20.txt\")\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 0.909 Gb\n",
      "all cohesion probabilities was computed. # words = 223348\n",
      "all branching entropies was computed # words = 361598\n",
      "all accessor variety was computed # words = 361598\n"
     ]
    }
   ],
   "source": [
    "word_extractor = WordExtractor()\n",
    "word_extractor.train(corpus)\n",
    "word_score_table = word_extractor.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(word_score_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'삼성디스플레이가': 2, 'OLED(올레드)': 1, '노트북에서': 1, '‘다크': 1, '모드’를': 1, '적용하면': 1, '디스플레이': 2, '소비전력': 1, '25%를': 1, '아낄': 1, '수': 2, '있다고': 1, '6일': 1, '밝혔다.': 1, 'IT': 2, '업계에서': 1, '노트북의': 2, '배터리': 3, '성능을': 1, '평가할': 1, '때': 2, '주로': 1, '사용하는': 1, '기준인': 1, '‘모바일': 1, '마크': 1, '2018’1)': 1, '시나리오에': 1, '따라': 1, 'OLED': 2, '사용시간을': 1, '측정한': 1, '결과,': 1, '화이트': 1, '모드에서는': 2, '9.9시간,': 1, '다크': 8, '11시간으로': 1, '나타났다.': 1, '모드를': 4, '적용했을': 1, '화면을': 1, '구성하는': 1, '전체': 1, '픽셀': 1, '중': 1, '작동하는': 1, '픽셀의': 1, '비율을': 1, '나타내는': 1, '‘OPR(On': 1, 'Pixel': 1, 'Ratio)’이': 1, '감소하면서': 1, '소비전력이': 3, '25%': 1, '줄었고,': 1, '이로': 2, '인해': 2, '노트북': 4, '사용': 1, '시간은': 1, '약': 1, '1시간': 1, '정도': 1, '늘었다.': 1, '이런': 2, '절약': 1, '효과는': 1, '디스플레이가': 1, 'OLED일': 1, '경우에만': 1, '가능하다.': 1, '삼성디스플레이는': 2, '“픽셀': 1, '하나하나가': 1, '스스로': 1, '빛을': 1, '내는': 1, 'OLED는': 1, '픽셀을': 1, '끄는': 1, '방식으로': 1, '검은': 2, '색을': 1, '표현하는데,': 1, '이때는': 1, '일종의': 1, '암전': 1, '상태이기': 1, '때문에': 1, '현저하게': 1, '줄어든다“며': 1, '”반면': 1, 'LCD(액정표시장치)는': 1, '검은색을': 1, '표현할': 2, '때도': 1, '백라이트는': 1, '항상': 1, '켜져': 1, '있어': 1, '‘트루': 1, '블랙’을': 1, '없을': 1, '뿐만': 1, '아니라': 1, '적용해도': 1, '떨어지지': 1, '않는다”고': 1, '말했다.': 2, '모드는': 1, '화면에서': 1, '정보를': 1, '담고': 1, '있지': 1, '않은': 1, '바탕을': 1, '어둡게': 1, '바꾸는UI(사용자': 1, '환경)': 1, '디자인이다.': 1, '‘밝은': 1, '화면에': 2, '글자’': 1, '대신': 1, '‘어두운': 1, '흰': 1, '글자’가': 1, '표출된다.': 1, '최근': 1, '애플,': 1, '마이크로소프트,': 1, '구글,': 1, '어도비': 1, '등': 1, '글로벌': 1, '기업들이': 1, '자사의': 1, '서비스에': 1, '앞다퉈': 1, '지원하고': 1, '있으며': 2, '지난': 1, '28일에는': 1, '삼성전자가': 1, '윈도우': 1, '설정에서': 1, '기본': 1, '적용한': 1, '신제품': 1, '‘갤럭시': 2, '북': 2, '프로’과': 1, '프로': 1, '360’을': 1, '공개했다.': 1, '백지호': 1, '삼성디스플레이': 1, '중소형': 1, '전략마케팅실장(부사장)은': 1, '“스마트폰,': 1, '같은': 1, '모바일': 1, '기기의': 1, '휴대성을': 1, '높이기': 1, '위해서는': 1, '저전력': 1, '기술이': 1, '중요한데': 1, 'OLED와': 1, '모드가': 1, '문제를': 1, '해결하는': 1, '데': 1, '효과적인': 1, '솔루션”이라며': 1, '“특히': 1, '모드로': 1, '기기를': 1, '사용하면': 1, '블루': 1, '라이트가': 1, '감소해': 1, '눈에도': 1, '이롭다”고': 1, '올해': 2, '초부터': 1, '13.3형부터': 1, '16형까지': 1, '노트북용': 1, '라인업을': 1, '10종': 1, '이상으로': 1, '확대해': 1, 'LCD': 1, '중심의': 1, '시장을': 1, '적극적으로': 1, '공략하고': 1, '비대면': 1, '및': 1, '고사양': 1, '수요': 1, '증가로': 1, '작년': 1, '대비': 1, '판매량이': 1, '5배가량': 1, '늘어날': 1, '것으로': 1, '전망하고': 1, '있다.': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-dd6e9a45578a>:6: UserWarning: loadtxt: Empty input file: \"<_io.TextIOWrapper name='F:/news/newss/news1.txt' mode='r' encoding='utf-8'>\"\n",
      "  news = np.loadtxt(f, delimiter = ',', dtype = 'str')\n"
     ]
    }
   ],
   "source": [
    "from soynlp.tokenizer import MaxScoreTokenizer\n",
    "scores = {word:score.cohesion_forward for word, score in word_score_table.items()}\n",
    "maxscore_tokenizer = MaxScoreTokenizer(scores=scores)\n",
    "f = open(\"F:/news/newss/news1.txt\", 'r',encoding=\"utf-8\")\n",
    "text=f.read()\n",
    "news = np.loadtxt(f, delimiter = ',', dtype = 'str')\n",
    "#news = np.loadtxt('', delimiter = ',', skiprows = 1, dtype = 'str')\n",
    "\n",
    "#print(news)\n",
    " \n",
    "#print(text)\n",
    "wordList = text.split()\n",
    " \n",
    "wordCount = {}\n",
    "sort_keys=[]\n",
    "for word in wordList:\n",
    "    # Get 명령어를 통해, Dictionary에 Key가 없으면 0리턴\n",
    "    wordCount[word] = wordCount.get(word, 0) + 1 \n",
    "    keys = sorted(wordCount.keys())\n",
    "    sort_keys.append(keys)\n",
    "    \n",
    "\n",
    "print(wordCount)\n",
    "\n",
    "#sort_keys = sorted(sort_keys)\n",
    "#for word in keys:\n",
    "#    wordCount[word] >= np.average(sort_keys)\n",
    "\n",
    "\n",
    "#for word in keys:\n",
    "   #wordCount[word] >= \n",
    "    \n",
    "#for word in keys:\n",
    "#    print(word + ':' + str(wordCount[word]))\n",
    "          \n",
    "#a=maxscore_tokenizer.tokenize(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 3, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 8, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 2, 2, 4, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "vector = np.array(wordCount.values())\n",
    "#print(list(wordCount.values()))\n",
    "a = list(wordCount.values())\n",
    "print(a)\n",
    "#print(wordCount.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['삼성' '전자' '가' \"'\" '패밀리' \"허브'\" '기능을' '갖춘' '비' '스포' '크(BESPOKE)' '냉장고'\n",
      " '신제품을' '3일' '출시한다' '.' '비' '스포' '크' '냉장고' '패밀리' '허브는' '인공지능' '(AI)과' '사물'\n",
      " '인터넷' '(IoT)을' '접목해' '식자재' '관리' ',' '가족' '간' '커뮤니케이션' ',' '엔터테인먼트' ','\n",
      " '스마트' '홈' '등의' '기능을' '수행' '하는' '제품' '으로' ',' '그' '혁신' '성을' '인정' '받아' '20'\n",
      " '16년' '첫' '선을' '보인' '이래' '6년' '연속' 'CES' '혁신' '상을' '수상' '했다' '.' '이' '제품'\n",
      " '은' '특히' '식품' '자동' '인식,' '맞춤형' '식단' '추천' '등의' '기능을' '구현' '하는' \"'\" '푸드'\n",
      " \"AI'\" '기술이' '적용' '돼' '식재료를' '편리하게' '관리' '할' '수' '있다' '는' '점' '에서' '호평'\n",
      " '받아' '왔다' '.' '비' '스포' '크' '패밀리' '허브는' '고도화' '된' '식품' '자동' '인식' '기술로'\n",
      " '보관' '중인' '다양한' '식재료를' '스스로' '파악하' '며,' '인식된' '식재료' '는' \"'\" '푸드' '리스트'\n",
      " \"'에\" '추가' '해' '관리' '할' '수' '있다' '.' '푸드' '리스트' '의' '식재료' '나' '가족' '구성'\n",
      " '원의' '음식' '취향을' '바탕' '으로' '최적의' '식단과' '레시피를' '제안' '하는' '기능의' '경우' ','\n",
      " '식재료' '선호도' '에서' '부터' '다이어트' ',' '영양' '등' '총' '7가지로' '세분화된' '옵션' '을'\n",
      " '선택할' '수' '있어' '한층' '진화된' '개인' '맞춤형' '식단을' '제공한다' '.' '추천' '레시피' '에서'\n",
      " '제공하는' '조리' '모드나' '시간' ',' '온도는' '삼성' '직화오븐이나' '전자레인지' '로' '곧바로' '전송할'\n",
      " '수' '있고,' '필요' '한' '식재료' '는' '이마트' '몰' '앱을' '통해' '간편하게' '온라인' '으로' '주문할'\n",
      " '수' '있어' '하나로' '연결' '되는' '주방' '경험을' '가능' '케' '해' '준다' '.' '비' '스포' '크'\n",
      " '패밀리' '허브를' '모바일' '스마트' '싱스' '앱의' \"'\" '스마트' '싱스' '쿠킹(SmartTh' 'ings'\n",
      " \"Cooking)'\" '서비스' '와' '연동해' '놓으면' ',' '언제' '어디서나' '패밀리' '허브' '기능을' '활용'\n",
      " '할' '수' '있다' '.' '냉장고' '내부의' '카메라' '를' '활용' '한' \"'뷰\" '인사이드' '(View'\n",
      " \"Inside)'\" '기능도' '업그레이드' '됐다' '.' '기존처럼' '앱을' '클릭하' '거나' '두' '손가락' '으로'\n",
      " '쓸어' '내리는' '방식' '은' '물론' ',' '꺼진' '화면을' '두' '번' '탭' '하는' '것만' '으로' '도'\n",
      " '간편하게' '냉장고' '내부를' '확인' '할' '수' '있다' '.' '원하는' '서비스를' '더욱' '빠르' '고' '쉽게'\n",
      " '찾아' '사용할' '수' '있도록' 'UX(' '사용' '경험)도' '새롭게' '디자' '인' '됐다' '.' '제품' '외부에'\n",
      " '있는' '스크린' '에' '쿠킹·' '패밀리' '·' '엔터테인먼트' '·' '스마트' '홈' '등' '4개의' '서비스'\n",
      " '보드를' '구성' '하고' '주요' '앱들을' '연관' '카테고리에' '통합' '배치' '했으며' ',' '앱을' '실행'\n",
      " '하지' '않고' '도' '바로' '사용' '가능한' '대형' '위젯을' '추가' '했다' '.' '각' '서비스' '보드는'\n",
      " '사진' '이나' '메모' ',' '동' '영상' '등을' '활용' '해' '자유롭게' '꾸밀' '수' '있고,' '사용자가'\n",
      " '개인' '취향을' '반영' '해' '보드를' '추가' '하는' '것도' '가능하다' '.' '제품을' '사용' '하지' '않을'\n",
      " '때' '그림이나' '사진' '을' '보여' '줘' '공간에' '품격을' '더' '하는' \"'\" '커버' '스크린' \"'\"\n",
      " '기능도' '강화' '됐다' '.' '이번' '신제품' '에는' '세계' '각국' '아티스트들의' '고전' '미술과' '현대'\n",
      " '미술' '작품' '1' '20' '여점' '이상을' '탑재' '했고,' '각' '작품' '에' '대한' '설명' '을' '음성'\n",
      " '이나' '텍스' '트로' '제공하는' \"'아뜰리에(Atel\" \"ier)'\" '기능을' '추가' '했다' '.' '스마트'\n",
      " '싱스(SmartTh' 'ings)' '서비스' '와' '연동' '되는' \"'\" '스마트' \"홈'\" '기능도' '개선' '됐다'\n",
      " '.' '▲가전' '제품을' '진단,' '관리하는' \"'홈케어\" '매니저' \"'\" '▲공기' '질을' '통합' '관리' '하고'\n",
      " '제어' '하는' \"'\" '스마트' '싱스' \"에어'\" '▲' '에너지' '사' '용량' '모니터링' '과' '절전' '가이드'\n",
      " '를' '제공하는' \"'\" '스마트' '싱스' '에너지' \"'\" '▲보안' '관리를' '위한' \"'\" '스마트' '싱스' '홈'\n",
      " '모니터' \"'\" '등을' '비' '스포' '크' '패밀리' '허브' '스크린' '을' '통해' '손' '쉽게' '사용할' '수'\n",
      " '있다' '.' '취향에' '따라' '선택' '가능한' '도어' '패널' '은' '총' '2' '3가지' '색상' '으로'\n",
      " '선보이' '며,' '터치스크린' '이' '포함' '된' '패널' '은' '글램' '네이' '비와' '글램' '화이트' ','\n",
      " '글램' '딥차콜' '3가지로' '운영' '된다' '.' '출고가는' '용량이' '나' '도어' '패널' '사양에' '따라'\n",
      " '35' '9만원' '에서' '542만원이다.' '이달래' '삼성' '전자' '생활' '가전' '사업' '부' '상무는' '\"비'\n",
      " '스포' '크' '패밀리' '허브는' 'AI' '기반' '다양한' '기능에' '비' '스포' '크' '디자' '인' '까지'\n",
      " '적용해' '진정한' '소비자' '맞춤형' '냉장고' '로' '진화한' '제품' '\"이' '라며' '\"이' '제품이' '지닌'\n",
      " '고유한' '가치를' '통해' '소비자들에게' '차별화된' '주방' '경험을' '제공' '할' '것\"이' '라고' '말했다' '.']\n"
     ]
    }
   ],
   "source": [
    "new_array = np.array(a)\n",
    "print(new_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 files belonging to 0 classes.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Input 'filename' of 'ReadFile' Op has type float32 that does not match expected type of string.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    516\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m             values = ops.convert_to_tensor(\n\u001b[0m\u001b[0;32m    518\u001b[0m                 \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1506\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1507\u001b[1;33m       raise ValueError(\n\u001b[0m\u001b[0;32m   1508\u001b[0m           \u001b[1;34m\"Tensor conversion requested dtype %s for Tensor with dtype %s: %r\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Tensor conversion requested dtype string for Tensor with dtype float32: <tf.Tensor 'args_0:0' shape=() dtype=float32>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-b75946b7eaee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnews_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"F:/news/newss\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m directory_txt = tf.keras.preprocessing.text_dataset_from_directory(\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mnews_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"inferred\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mlabel_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"int\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\preprocessing\\text_dataset.py\u001b[0m in \u001b[0;36mtext_dataset_from_directory\u001b[1;34m(directory, labels, label_mode, class_names, batch_size, max_length, shuffle, seed, validation_split, subset, follow_links)\u001b[0m\n\u001b[0;32m    154\u001b[0m       file_paths, labels, validation_split, subset)\n\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m   dataset = paths_and_labels_to_dataset(\n\u001b[0m\u001b[0;32m    157\u001b[0m       \u001b[0mfile_paths\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfile_paths\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m       \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\preprocessing\\text_dataset.py\u001b[0m in \u001b[0;36mpaths_and_labels_to_dataset\u001b[1;34m(file_paths, labels, label_mode, num_classes, max_length)\u001b[0m\n\u001b[0;32m    176\u001b[0m   \u001b[1;34m\"\"\"Constructs a dataset of text strings and labels.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m   \u001b[0mpath_ds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_paths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m   string_ds = path_ds.map(\n\u001b[0m\u001b[0;32m    179\u001b[0m       lambda x: path_to_string_content(x, max_length))\n\u001b[0;32m    180\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mlabel_mode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[0;32m   1803\u001b[0m     \"\"\"\n\u001b[0;32m   1804\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1805\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1806\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1807\u001b[0m       return ParallelMapDataset(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[0;32m   4201\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_use_inter_op_parallelism\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4202\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_preserve_cardinality\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4203\u001b[1;33m     self._map_func = StructuredFunctionWrapper(\n\u001b[0m\u001b[0;32m   4204\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4205\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m   3523\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3524\u001b[0m         \u001b[1;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3525\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3526\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3527\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3049\u001b[0m       \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minputs\u001b[0m \u001b[0mto\u001b[0m \u001b[0mspecialize\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3050\u001b[0m     \"\"\"\n\u001b[1;32m-> 3051\u001b[1;33m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0m\u001b[0;32m   3052\u001b[0m         *args, **kwargs)\n\u001b[0;32m   3053\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3017\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3018\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3019\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3020\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3021\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3194\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3195\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3196\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3197\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3198\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 990\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   3516\u001b[0m           attributes=defun_kwargs)\n\u001b[0;32m   3517\u001b[0m       \u001b[1;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=missing-docstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3518\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3519\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3520\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   3451\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3453\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3454\u001b[0m       \u001b[1;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3455\u001b[0m       \u001b[1;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    665\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 667\u001b[1;33m           \u001b[1;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    668\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_allowlisted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 396\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m   \u001b[1;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 478\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    479\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\preprocessing\\text_dataset.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    177\u001b[0m   \u001b[0mpath_ds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_paths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m   string_ds = path_ds.map(\n\u001b[1;32m--> 179\u001b[1;33m       lambda x: path_to_string_content(x, max_length))\n\u001b[0m\u001b[0;32m    180\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mlabel_mode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[0mlabel_ds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels_to_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_mode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\preprocessing\\text_dataset.py\u001b[0m in \u001b[0;36mpath_to_string_content\u001b[1;34m(path, max_length)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpath_to_string_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m   \u001b[0mtxt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[0mtxt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstring_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\u001b[0m in \u001b[0;36mread_file\u001b[1;34m(filename, name)\u001b[0m\n\u001b[0;32m    568\u001b[0m   \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m     _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0m\u001b[0;32m    571\u001b[0m         \"ReadFile\", filename=filename, name=name)\n\u001b[0;32m    572\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    542\u001b[0m                     (input_name, op_type_name, observed))\n\u001b[0;32m    543\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtypes_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDT_INVALID\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 544\u001b[1;33m             raise TypeError(\"%s expected type of %s.\" %\n\u001b[0m\u001b[0;32m    545\u001b[0m                             (prefix, dtypes.as_dtype(input_arg.type).name))\n\u001b[0;32m    546\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Input 'filename' of 'ReadFile' Op has type float32 that does not match expected type of string."
     ]
    }
   ],
   "source": [
    "news_dir = \"F:/news/newss\"\n",
    "directory_txt = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    news_dir,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"int\",\n",
    "    class_names=None,\n",
    "    batch_size=32,\n",
    "    max_length=None,\n",
    "    shuffle=True,\n",
    "    seed=None,\n",
    "    validation_split=None,\n",
    "    subset=None,\n",
    "    follow_links=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer=tf.keras.preprocessing.text.Tokenizer(\n",
    "    num_words=None,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "    lower=True,\n",
    "    split=\" \",\n",
    "    char_level=False,\n",
    "    oov_token=None,\n",
    "    document_count=0\n",
    ")\n",
    "tokenizer.fit_on_texts(news)\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 17]\n",
      "[17]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 17]\n"
     ]
    }
   ],
   "source": [
    "train_sentences = []\n",
    "train_labels = []\n",
    "test_sentences = []\n",
    "test_labels = []\n",
    "\n",
    "for s in text:\n",
    "    train_sentences.append(s)\n",
    "#    train_labels.append(l.numpy())\n",
    "\n",
    "for s in text:\n",
    "    test_sentences.append(s)\n",
    "#    test_labels.append(l.numpy())\n",
    "\n",
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "max_length = 120\n",
    "trunc_type = 'post'\n",
    "oov_tok = \"<OOV>\"\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "padded = pad_sequences(sequences, maxlen=max_length, truncating=trunc_type)\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_length)\n",
    "\n",
    "print(sequences[0])\n",
    "print(padded[0])\n",
    "\n",
    "print(test_sequences[0])\n",
    "print(test_padded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example data\n",
    "sentences = [['I', 'feel', 'hungry'],\n",
    "     ['tensorflow', 'is', 'very', 'difficult'],\n",
    "     ['tensorflow', 'is', 'a', 'framework', 'for', 'deep', 'learning'],\n",
    "     ['tensorflow', 'is', 'very', 'fast', 'changing']]\n",
    "pos = [['pronoun', 'verb', 'adjective'],\n",
    "     ['noun', 'verb', 'adverb', 'adjective'],\n",
    "     ['noun', 'verb', 'determiner', 'noun', 'preposition', 'adjective', 'noun'],\n",
    "     ['noun', 'verb', 'adverb', 'adjective', 'verb']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a token dictionary for word\n",
    "word_list = sum(sentences, [])\n",
    "word_list = sorted(set(word_list))\n",
    "word_list = ['<pad>'] + word_list\n",
    "word2idx = {word : idx for idx, word in enumerate(word_list)}\n",
    "idx2word = {idx : word for idx, word in enumerate(word_list)}\n",
    "\n",
    "print(word2idx)\n",
    "print(idx2word)\n",
    "print(len(idx2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a token dictionary for part of speech\n",
    "pos_list = sum(pos, [])\n",
    "pos_list = sorted(set(pos_list))\n",
    "pos_list = ['<pad>'] + pos_list\n",
    "pos2idx = {pos : idx for idx, pos in enumerate(pos_list)}\n",
    "idx2pos = {idx : pos for idx, pos in enumerate(pos_list)}\n",
    "\n",
    "print(pos2idx)\n",
    "print(idx2pos)\n",
    "print(len(pos2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting sequence of tokens to sequence of indices\n",
    "max_sequence = 10\n",
    "x_data = list(map(lambda sentence : [word2idx.get(token) for token in sentence], sentences))\n",
    "y_data = list(map(lambda sentence : [pos2idx.get(token) for token in sentence], pos))\n",
    "\n",
    "# padding the sequence of indices\n",
    "x_data = pad_sequences(sequences = x_data, maxlen = max_sequence, padding='post')\n",
    "x_data_mask = ((x_data != 0) * 1).astype(np.float32)\n",
    "x_data_len = list(map(lambda sentence : len(sentence), sentences))\n",
    "\n",
    "y_data = pad_sequences(sequences = y_data, maxlen = max_sequence, padding='post')\n",
    "\n",
    "# checking data\n",
    "print(x_data, x_data_len)\n",
    "print(x_data_mask)\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating bidirectional rnn for \"many to many\" sequence tagging\n",
    "num_classes = len(pos2idx)\n",
    "hidden_dim = 10\n",
    "\n",
    "input_dim = len(word2idx)\n",
    "output_dim = len(word2idx)\n",
    "one_hot = np.eye(len(word2idx))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.InputLayer(input_shape=(max_sequence,)))\n",
    "model.add(layers.Embedding(input_dim=input_dim, output_dim=output_dim, mask_zero=True,\n",
    "                                 trainable=False, input_length=max_sequence,\n",
    "                                 embeddings_initializer=keras.initializers.Constant(one_hot)))\n",
    "model.add(layers.Bidirectional(keras.layers.SimpleRNN(units=hidden_dim, return_sequences=True)))\n",
    "model.add(layers.TimeDistributed(keras.layers.Dense(units=num_classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "# creating loss function\n",
    "def loss_fn(model, x, y, x_len, max_sequence):\n",
    "    masking = tf.sequence_mask(x_len, maxlen=max_sequence, dtype=tf.float32)\n",
    "    valid_time_step = tf.cast(x_len,dtype=tf.float32)\n",
    "    sequence_loss = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        y_true=y, y_pred=model(x), from_logits=True) * masking\n",
    "    sequence_loss = tf.reduce_sum(sequence_loss, axis=-1) / valid_time_step\n",
    "    sequence_loss = tf.reduce_mean(sequence_loss)\n",
    "    return sequence_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating and optimizer\n",
    "lr = 0.1\n",
    "epochs = 30\n",
    "batch_size = 2\n",
    "opt = tf.keras.optimizers.Adam(learning_rate = lr)\n",
    "\n",
    "# generating data pipeline\n",
    "tr_dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data, x_data_len))\n",
    "tr_dataset = tr_dataset.shuffle(buffer_size=4)\n",
    "tr_dataset = tr_dataset.batch(batch_size = 2)\n",
    "\n",
    "print(tr_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n",
      "{'<pad>': 0, 'I': 1, 'a': 2, 'changing': 3, 'deep': 4, 'difficult': 5, 'fast': 6, 'feel': 7, 'for': 8, 'framework': 9, 'hungry': 10, 'is': 11, 'learning': 12, 'tensorflow': 13, 'very': 14}\n",
      "{0: '<pad>', 1: 'I', 2: 'a', 3: 'changing', 4: 'deep', 5: 'difficult', 6: 'fast', 7: 'feel', 8: 'for', 9: 'framework', 10: 'hungry', 11: 'is', 12: 'learning', 13: 'tensorflow', 14: 'very'}\n",
      "15\n",
      "{'<pad>': 0, 'adjective': 1, 'adverb': 2, 'determiner': 3, 'noun': 4, 'preposition': 5, 'pronoun': 6, 'verb': 7}\n",
      "{0: '<pad>', 1: 'adjective', 2: 'adverb', 3: 'determiner', 4: 'noun', 5: 'preposition', 6: 'pronoun', 7: 'verb'}\n",
      "8\n",
      "[[ 1  7 10  0  0  0  0  0  0  0]\n",
      " [13 11 14  5  0  0  0  0  0  0]\n",
      " [13 11  2  9  8  4 12  0  0  0]\n",
      " [13 11 14  6  3  0  0  0  0  0]] [3, 4, 7, 5]\n",
      "[[1. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0.]]\n",
      "[[6 7 1 0 0 0 0 0 0 0]\n",
      " [4 7 2 1 0 0 0 0 0 0]\n",
      " [4 7 3 4 5 1 4 0 0 0]\n",
      " [4 7 2 1 7 0 0 0 0 0]]\n",
      "<BatchDataset shapes: ((None, 10), (None, 10), (None,)), types: (tf.int32, tf.int32, tf.int32)>\n",
      "epoch :   5, tr_loss : 0.012\n",
      "epoch :  10, tr_loss : 0.001\n",
      "epoch :  15, tr_loss : 0.000\n",
      "epoch :  20, tr_loss : 0.000\n",
      "epoch :  25, tr_loss : 0.000\n",
      "epoch :  30, tr_loss : 0.000\n",
      "[['pronoun', 'verb', 'adjective', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'],\n",
      " ['noun', 'verb', 'adverb', 'adjective', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'],\n",
      " ['noun', 'verb', 'determiner', 'noun', 'preposition', 'adjective', 'noun', '<pad>', '<pad>', '<pad>'],\n",
      " ['noun', 'verb', 'adverb', 'adjective', 'verb', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']]\n",
      "[['pronoun', 'verb', 'adjective'],\n",
      " ['noun', 'verb', 'adverb', 'adjective'],\n",
      " ['noun', 'verb', 'determiner', 'noun', 'preposition', 'adjective', 'noun'],\n",
      " ['noun', 'verb', 'adverb', 'adjective', 'verb']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2e3ef9fd670>]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training\n",
    "tr_loss_hist = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    avg_tr_loss = 0\n",
    "    tr_step = 0\n",
    "\n",
    "    for x_mb, y_mb, x_mb_len in tr_dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            tr_loss = loss_fn(model, x=x_mb, y=y_mb, x_len=x_mb_len, max_sequence=max_sequence)\n",
    "        grads = tape.gradient(target=tr_loss, sources=model.variables)\n",
    "        opt.apply_gradients(grads_and_vars=zip(grads, model.variables))\n",
    "        avg_tr_loss += tr_loss\n",
    "        tr_step += 1\n",
    "    else:\n",
    "        avg_tr_loss /= tr_step\n",
    "        tr_loss_hist.append(avg_tr_loss)\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print('epoch : {:3}, tr_loss : {:.3f}'.format(epoch + 1, avg_tr_loss))\n",
    "\n",
    "yhat = model.predict(x_data)\n",
    "yhat = np.argmax(yhat, axis=-1) * x_data_mask\n",
    "\n",
    "pprint(list(map(lambda row : [idx2pos.get(elm) for elm in row],yhat.astype(np.int32).tolist())), width = 120)\n",
    "pprint(pos)\n",
    "\n",
    "plt.plot(tr_loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Attachments",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import JPype1\n",
    "from konlpy.tag import Hannanum\n",
    "from konlpy.utils import pprint\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from datetime import datetime, timedelta\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "import tensorflow.keras as keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import requests\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "os.chdir('F:/news/newss')\n",
    "# ## 주가 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file():\n",
    "    stock = pd.read_csv(\"sk하이닉스.csv\")\n",
    "    stock['date'] = pd.to_datetime(stock['date'], format ='%Y-%m-%d')\n",
    "    for i in range(len(stock)):\n",
    "        stock['per'][i]=(float(stock['per'][i].replace(\"%\",\"\"))/100)\n",
    "\n",
    "    # # 뉴스 url읽기\n",
    "    url = pd.read_csv(\"crawling_news2.txt\", header = None, names = ['date','href'])\n",
    "    url['per']= \"A\"\n",
    "    return url, stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-411-91f608ca0a0c>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stock['per'][i]=(float(stock['per'][i].replace(\"%\",\"\"))/100)\n"
     ]
    }
   ],
   "source": [
    "url, stock = read_file()\n",
    "print(\"read_file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "#전처리\n",
    "def Pretreatment(url,stock):\n",
    "    delet_num=[]\n",
    "    today = pd.Timestamp.now()\n",
    "    today = str(today)[:10]\n",
    "    today = pd.to_datetime(today, format ='%Y-%m-%d')\n",
    "    for i in range(len(url)):\n",
    "        url['date'][i] = str(url['date'][i])[:8]\n",
    "        url['date'][i] = pd.to_datetime(url['date'][i], format ='%y.%m.%d')\n",
    "        url['date'][i] = pd.Timestamp(url['date'][i]).date()\n",
    "        while url['date'][i] not in list(stock['date']):\n",
    "            url['date'][i]=url['date'][i]+timedelta(days=1)\n",
    "#         if url['date'][i] == today:\n",
    "#             delet_num.append(i)\n",
    "#             continue\n",
    "        for j in range(len(stock)):\n",
    "            if url['date'][i] == stock['date'][j]:\n",
    "                url['per'][i] = stock['per'][j-1] #다음날 주가\n",
    "#     url=np.delete(url, (delet_num), axis = 0)\n",
    "    te4 = pd.DataFrame(url)\n",
    "    te4 =te4.drop_duplicates([\"date\"])\n",
    "    q1 = te4[\"per\"].quantile(.25)\n",
    "    q3 = te4[\"per\"].quantile(.75)\n",
    "    for i in range(len(url)):\n",
    "        if url['per'][i] > q3:\n",
    "            url['per'][i] = 0\n",
    "        elif url['per'][i] <= q3 and url['per'][i]>0:\n",
    "            url['per'][i] = 1\n",
    "        elif url['per'][i] <= 0 and url['per'][i]>q1:\n",
    "            url['per'][i] = 2\n",
    "        else:\n",
    "            url['per'][i] = 3\n",
    "    return url\n",
    "\n",
    "    # ### 퍼센트 원 핫 인코딩\n",
    "\n",
    "    # ##### 원핫 인코딩 y값 제거해야 한다 함\n",
    "\n",
    "    # from tensorflow.keras.utils import to_categorical\n",
    "    # #float(te4[\"per\"][1])\n",
    "    # url['per'] = list(to_categorical(url['per']))\n",
    "    #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretreatment\n"
     ]
    }
   ],
   "source": [
    "url = Pretreatment(url,stock)\n",
    "print(\"Pretreatment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test와 train데이터 분리\n",
    "\n",
    "# # 전처리 및 토큰화\n",
    "def morpheme(data):\n",
    "    news_group=[]\n",
    "    j=0\n",
    "    for i in range(len(data)):\n",
    "        webpage = requests.get(data['href'][i])\n",
    "        soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "        soup = soup.select_one('#newsViewArea').get_text()\n",
    "        soup = re.sub('[^ㄱ-ㅎㅏ-ㅣ가-힣A-Za-z ]', '', soup)\n",
    "        if i == 0 :\n",
    "            news_group_date = data['date'][i]\n",
    "            news_group_content = soup\n",
    "            count = 1\n",
    "            news_group = pd.DataFrame([news_group_date,news_group_content,data['per'][i],count],['date','content','per','count']).transpose()\n",
    "            continue\n",
    "        if news_group['date'][j] == data['date'][i]:\n",
    "            news_group['content'][j] = news_group['content'][j] + soup\n",
    "            count + 1\n",
    "            news_group['count'][j] = count\n",
    "        else :\n",
    "            j = j + 1\n",
    "            count = 1\n",
    "            a = {'date':data['date'][i],'content':soup,'per':data['per'][i],'count':count}\n",
    "            news_group = news_group.append(a,ignore_index=True)\n",
    "    return news_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "morpheme\n"
     ]
    }
   ],
   "source": [
    "all_data=morpheme(url)\n",
    "print(\"morpheme\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data=all_data.drop_duplicates(['content'], keep = 'first')\n",
    "all_data.reset_index()\n",
    "all_data.to_csv('all_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv('all_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>per</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2021-05-17</td>\n",
       "      <td>삼성전자의 국내 직원 수가 만명을 돌파하며 역대 최대치를 기록했다일 삼성전자는 분기...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-05-16</td>\n",
       "      <td>올해 개인투자자가 유가증권시장에서 조원 넘게 순매수한 것으로 집계됐다 이는 지난해...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2021-05-14</td>\n",
       "      <td>삼성바이오로직스 주가가 일 사상 최고가를 기록하며 시가총액에서도 위에 올라섰다 미...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2021-04-06</td>\n",
       "      <td>미국 대표 메모리반도체 업체 마이크론이 분기월 결산법인으로 년 월년 월에 해당 호실...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2021-04-05</td>\n",
       "      <td>공매도 재개 시점이 한 달 앞으로 다가오면서 동학개미들이 좌불안석이다 오는 월 일...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>252</td>\n",
       "      <td>2012-04-05</td>\n",
       "      <td>일본의 도시바가 googletagdisplaygoogledfpMCx 한국의 SK 하...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>253</td>\n",
       "      <td>2012-04-04</td>\n",
       "      <td>금융감독원이 금융권에서 빚이 많은 개 대기업 그룹을 주채무계열로 선정했습니다   선...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>254</td>\n",
       "      <td>2012-04-02</td>\n",
       "      <td>주식투자를 위해 가장 중요한 것은 투자 승률과 수익률입니다주식투자로 돈을 버는 주식...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>255</td>\n",
       "      <td>2012-03-30</td>\n",
       "      <td>SK하이닉스가 세계 위의 D램업체인 일본의 엘피다 인수를 추진합니다   SK하이닉스...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>259</td>\n",
       "      <td>2012-03-26</td>\n",
       "      <td>서울 핵안보정상회의 오늘 개막  핵테러 방지를 위한 서울 핵안보정상회의가 이틀간의...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>257 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0        date  \\\n",
       "0             0  2021-05-17   \n",
       "1             1  2021-05-16   \n",
       "2             2  2021-05-14   \n",
       "3             3  2021-04-06   \n",
       "4             4  2021-04-05   \n",
       "..          ...         ...   \n",
       "252         252  2012-04-05   \n",
       "253         253  2012-04-04   \n",
       "254         254  2012-04-02   \n",
       "255         255  2012-03-30   \n",
       "259         259  2012-03-26   \n",
       "\n",
       "                                               content  per  count  \n",
       "0    삼성전자의 국내 직원 수가 만명을 돌파하며 역대 최대치를 기록했다일 삼성전자는 분기...    0      1  \n",
       "1     올해 개인투자자가 유가증권시장에서 조원 넘게 순매수한 것으로 집계됐다 이는 지난해...    3      1  \n",
       "2     삼성바이오로직스 주가가 일 사상 최고가를 기록하며 시가총액에서도 위에 올라섰다 미...    2      1  \n",
       "3    미국 대표 메모리반도체 업체 마이크론이 분기월 결산법인으로 년 월년 월에 해당 호실...    1      1  \n",
       "4     공매도 재개 시점이 한 달 앞으로 다가오면서 동학개미들이 좌불안석이다 오는 월 일...    2      1  \n",
       "..                                                 ...  ...    ...  \n",
       "252  일본의 도시바가 googletagdisplaygoogledfpMCx 한국의 SK 하...    3      1  \n",
       "253  금융감독원이 금융권에서 빚이 많은 개 대기업 그룹을 주채무계열로 선정했습니다   선...    0      1  \n",
       "254  주식투자를 위해 가장 중요한 것은 투자 승률과 수익률입니다주식투자로 돈을 버는 주식...    0      1  \n",
       "255  SK하이닉스가 세계 위의 D램업체인 일본의 엘피다 인수를 추진합니다   SK하이닉스...    1      1  \n",
       "259   서울 핵안보정상회의 오늘 개막  핵테러 방지를 위한 서울 핵안보정상회의가 이틀간의...    1      1  \n",
       "\n",
       "[257 rows x 5 columns]"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      1\n",
      "1      1\n",
      "2      1\n",
      "3      1\n",
      "4      1\n",
      "      ..\n",
      "252    1\n",
      "253    1\n",
      "254    1\n",
      "255    1\n",
      "259    1\n",
      "Name: count, Length: 257, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(all_data['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_except_all(data):\n",
    "    tokken = []\n",
    "    tok=[]\n",
    "    hannanum=Hannanum()\n",
    "    for i in range(len(data)):\n",
    "        tokken.append(hannanum.pos(data['content'][i]))\n",
    "\n",
    "    # ### 토큰 중 가장 긴 토큰을 기준으로 반복 및 형태소 중 명사 동사 선택\n",
    "\n",
    "    lenA = []\n",
    "    for i in range(len(tokken)):\n",
    "        lenA.append(len(tokken[i]))\n",
    "    max(lenA)\n",
    "\n",
    "    \n",
    "    Stopword = pd.read_csv(\"한국어불용어100.txt\", header=None, names=['text','x','num'],delimiter = '\\t')\n",
    "    for i in range(len(tokken)):\n",
    "        all_tokken=[]\n",
    "        for j in range(lenA[i]):\n",
    "            if tokken[i][j][1] == 'N' and tokken[i][j][0] not in Stopword['text'].values:\n",
    "                all_tokken.append(tokken[i][j][0])\n",
    "        tok.append(all_tokken)\n",
    "    return tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data=all_data.reset_index()\n",
    "all_data['content'] = text_except_all(all_data)\n",
    "# print(\"text_except\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>per</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-05-17</td>\n",
       "      <td>[삼성전자, 국내, 직원, 만명, 돌파, 역대, 최대치, 기록했다, 삼성전자, 분기...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-05-16</td>\n",
       "      <td>[올해, 개인투자자, 유가증권시장, 조원, 순매수한, 집계, 지난해, 전체, 기간,...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2021-05-14</td>\n",
       "      <td>[삼성바이오로직스, 주가, 사상, 최고가, 기록, 시가총액, 위, 미국, 모더나, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2021-04-06</td>\n",
       "      <td>[미국, 대표, 메모리반도체, 업체, 마이크론, 분기월, 결산법인, 월년, 해당, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2021-04-05</td>\n",
       "      <td>[공매, 재개, 시점, 달, 동학개미들, 좌불안석이다, 삼성전자, 비롯, 대형주, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>2012-04-05</td>\n",
       "      <td>[일본, 도시바, 한국의, 하이닉스반도체, 엘피다메모리, 공동, 제안, 니혼게이자이...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>2012-04-04</td>\n",
       "      <td>[금융감독원, 금융권, 빚, 대기업, 그룹, 주채무계열, 선정, 선정, 대상, 금융...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>254</td>\n",
       "      <td>254</td>\n",
       "      <td>2012-04-02</td>\n",
       "      <td>[주식투자, 중요, 투, 승률, 수익률입니다주식투자, 돈, 주식투자자, 의, 비법,...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>2012-03-30</td>\n",
       "      <td>[SK하이닉스, 세계, 위, D램업체, 일본, 엘피다, 추진, SK하이닉스, 거래소...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>259</td>\n",
       "      <td>259</td>\n",
       "      <td>2012-03-26</td>\n",
       "      <td>[서울, 핵안보정상회, 오늘, 개막, 핵테러, 방지, 서울, 핵안보정상회의, 이틀간...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>257 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index  Unnamed: 0        date  \\\n",
       "0        0           0  2021-05-17   \n",
       "1        1           1  2021-05-16   \n",
       "2        2           2  2021-05-14   \n",
       "3        3           3  2021-04-06   \n",
       "4        4           4  2021-04-05   \n",
       "..     ...         ...         ...   \n",
       "252    252         252  2012-04-05   \n",
       "253    253         253  2012-04-04   \n",
       "254    254         254  2012-04-02   \n",
       "255    255         255  2012-03-30   \n",
       "256    259         259  2012-03-26   \n",
       "\n",
       "                                               content  per  count  \n",
       "0    [삼성전자, 국내, 직원, 만명, 돌파, 역대, 최대치, 기록했다, 삼성전자, 분기...    0      1  \n",
       "1    [올해, 개인투자자, 유가증권시장, 조원, 순매수한, 집계, 지난해, 전체, 기간,...    3      1  \n",
       "2    [삼성바이오로직스, 주가, 사상, 최고가, 기록, 시가총액, 위, 미국, 모더나, ...    2      1  \n",
       "3    [미국, 대표, 메모리반도체, 업체, 마이크론, 분기월, 결산법인, 월년, 해당, ...    1      1  \n",
       "4    [공매, 재개, 시점, 달, 동학개미들, 좌불안석이다, 삼성전자, 비롯, 대형주, ...    2      1  \n",
       "..                                                 ...  ...    ...  \n",
       "252  [일본, 도시바, 한국의, 하이닉스반도체, 엘피다메모리, 공동, 제안, 니혼게이자이...    3      1  \n",
       "253  [금융감독원, 금융권, 빚, 대기업, 그룹, 주채무계열, 선정, 선정, 대상, 금융...    0      1  \n",
       "254  [주식투자, 중요, 투, 승률, 수익률입니다주식투자, 돈, 주식투자자, 의, 비법,...    0      1  \n",
       "255  [SK하이닉스, 세계, 위, D램업체, 일본, 엘피다, 추진, SK하이닉스, 거래소...    1      1  \n",
       "256  [서울, 핵안보정상회, 오늘, 개막, 핵테러, 방지, 서울, 핵안보정상회의, 이틀간...    1      1  \n",
       "\n",
       "[257 rows x 6 columns]"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.to_csv('all_data_tokken.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv('all_data_tokken.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 16237\n",
      "등장 빈도가 1번 이하인 희귀 단어의 수: 9316\n",
      "단어 집합에서 희귀 단어의 비율: 57.37513087392991\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 7.103370974997904\n",
      "단어 집합의 크기 : 6922\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_data['content'])\n",
    "\n",
    "vocab_size = 1000  # 상위 500 단어만 사용\n",
    "tokenizer = Tokenizer(num_words = vocab_size + 1)\n",
    "tokenizer.fit_on_texts(all_data['content'])\n",
    "\n",
    "# print(tokenizer.word_index) #인덱스가 어떻게 부여됬는지(입력된 단어 순서)\n",
    "# print(tokenizer.word_counts) #상위 몇개 단어를 했을 때 어떻게 부여됬는지(입력된 단어 순서)\n",
    "\n",
    "def text_size(num):\n",
    "    threshold = num\n",
    "    total_cnt = len(tokenizer.word_index) # 단어의 수\n",
    "    rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "    total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "    rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "    # 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "    for key, value in tokenizer.word_counts.items():\n",
    "        total_freq = total_freq + value\n",
    "\n",
    "        # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "        if(value < threshold):\n",
    "            rare_cnt = rare_cnt + 1\n",
    "            rare_freq = rare_freq + value\n",
    "\n",
    "    print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "    print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "    print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "    print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\n",
    "\n",
    "    #단어수가 2개인 단어의 빈도가 6.1%라 유의미한 영향을 줄 수 있어 제외하지 않는다\n",
    "\n",
    "    # 전체 단어 개수 중 빈도수 2이하인 단어는 제거.\n",
    "    # 0번 패딩 토큰을 고려하여 + 1\n",
    "    vocab_size = total_cnt - rare_cnt + 1\n",
    "    print('단어 집합의 크기 :',vocab_size)\n",
    "\n",
    "\n",
    "    # ## 앞의 형태소분석을 붙여씀\n",
    "    # ### 불필요하게 주가를 넣는 부분이 있고 href에서 본문을 따오는 부분 함수화 고려\n",
    "\n",
    "text_size(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "clustering = tokenizer.texts_to_sequences(all_data['content'])\n",
    "clustering = pad_sequences(clustering, maxlen = 500)\n",
    "# 군집화 할 그룹의 갯수 정의\n",
    "n_clusters = 4\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters).fit(clustering)\n",
    "\n",
    "# trained labels and cluster centers\n",
    "labels = kmeans.labels_\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "# labels에 merge\n",
    "all_data['labels'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_test\n"
     ]
    }
   ],
   "source": [
    "def train_test(data):\n",
    "    train, test = train_test_split(data, test_size= 0.2, random_state=1234)\n",
    "\n",
    "    #인덱스 초기화\n",
    "    train = train.reset_index()\n",
    "    test = test.reset_index()\n",
    "    return train, test\n",
    "\n",
    "train, test = train_test(all_data)\n",
    "print(\"train_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205\n",
      "205\n"
     ]
    }
   ],
   "source": [
    "# train_data = morpheme(train)\n",
    "# test_data = morpheme(test)\n",
    "\n",
    "# train_data['content'] = text_except(train_data['content'])\n",
    "# test_data['content'] = text_except(test_data['content'])\n",
    "\n",
    "# X_train = train['content']\n",
    "# X_test = test['content']\n",
    "X_train = tokenizer.texts_to_sequences(train['content'])\n",
    "X_test = tokenizer.texts_to_sequences(test['content'])\n",
    "\n",
    "y_train = train['per']\n",
    "y_train = pd.DataFrame(y_train)\n",
    "y_test = test['per']\n",
    "y_test = pd.DataFrame(y_test)\n",
    "\n",
    "\n",
    "drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]\n",
    "# 빈 샘플들을 제거\n",
    "# X_train = np.delete(X_train, drop_train, axis=0)\n",
    "# y_train = np.delete(y_train, drop_train, axis=0)\n",
    "print(len(X_train))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "뉴스의 최대 길이 : 12\n",
      "뉴스의 평균 길이 : 0.06779661016949153\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ## 패딩\n",
    "\n",
    "def below_threshold_len(max_len, nested_list):\n",
    "  cnt = 0\n",
    "  for s in nested_list:\n",
    "    if(len(s) <= max_len):\n",
    "        cnt = cnt + 1\n",
    "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))*100))\n",
    "\n",
    "\n",
    "print('뉴스의 최대 길이 :',max(len(l) for l in all_data))\n",
    "print('뉴스의 평균 길이 :',sum(map(len, all_data))/len(url))\n",
    "plt.hist([len(s) for s in X_train], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 1200 이하인 샘플의 비율: 100.0\n"
     ]
    }
   ],
   "source": [
    "max_len = 1200\n",
    "below_threshold_len(max_len, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-02-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-02-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-06-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-10-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-05-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>2017-12-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>2015-02-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>2019-11-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>2020-04-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>2014-11-20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>205 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date\n",
       "0    2020-02-18\n",
       "1    2020-02-19\n",
       "2    2017-06-08\n",
       "3    2015-10-07\n",
       "4    2019-05-28\n",
       "..          ...\n",
       "200  2017-12-26\n",
       "201  2015-02-04\n",
       "202  2019-11-11\n",
       "203  2020-04-16\n",
       "204  2014-11-20\n",
       "\n",
       "[205 rows x 1 columns]"
      ]
     },
     "execution_count": 741,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train = pad_sequences(X_train, maxlen = max_len)\n",
    "X_test = pad_sequences(X_test, maxlen = max_len)\n",
    "\n",
    "\n",
    "\n",
    "X_train2 = pd.DataFrame(X_train)\n",
    "X_train2=X_train2.astype('float64')\n",
    "X_test2 = pd.DataFrame(X_test)\n",
    "X_test2=X_test2.astype('float64')\n",
    "\n",
    "\n",
    "\n",
    "y_train=y_train.astype('float64')\n",
    "y_test=y_test.astype('float64')\n",
    "\n",
    "#------------------3D DataFrame--------------------bidirectional\n",
    "# threeD_X_train2 = pd.DataFrame(date = train['date'],content = X_train2,rabels = train['labels'])\n",
    "td = pd.DataFrame(train['date'])\n",
    "la = pd.DataFrame(train['labels'].astype(np.float64))\n",
    "# se = pd.Series(X_train)\n",
    "# X_train3 = pd.DataFrame({'content':X_train.values})\n",
    "threeD_X_train2 = pd.concat([X_train2,la],ignore_index=True,axis=1)\n",
    "threeD_X_train2=threeD_X_train2.astype(np.float64)\n",
    "threeD_X_train2 = np.array(threeD_X_train2).reshape(threeD_X_train2.shape[0], threeD_X_train2.shape[1], 1)\n",
    "# ,'date','content','labels'.transpose()\n",
    "\n",
    "#----------------------------------------\n",
    "\n",
    "# X_train2 = np.array(X_train2).reshape(X_train2.shape[0], X_train2.shape[1], 1)\n",
    "# y_train = np.array(y_train).reshape(y_train.shape[0], y_train.shape[1], 1)\n",
    "# X_test2 = np.array(X_test2).reshape(X_test2.shape[0], X_test2.shape[1], 1)\n",
    "# y_test = np.array(y_test).reshape(y_test.shape[0], y_test.shape[1], 1)\n",
    "\n",
    "\n",
    "# #### loss = sparse_categorical_crossentropy은 y값을 원핫인코딩하지 않는다\n",
    "# y_train = to_categorical(y_train, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0, ..., 22, 23, 40],\n",
       "       [ 0,  0,  0, ..., 22, 23, 40],\n",
       "       [ 0,  0,  0, ..., 22, 23, 40],\n",
       "       ...,\n",
       "       [ 0,  0,  0, ..., 22, 23, 40],\n",
       "       [ 0,  0,  0, ..., 22, 23, 40],\n",
       "       [ 0,  0,  0, ..., 22, 23, 40]])"
      ]
     },
     "execution_count": 743,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205"
      ]
     },
     "execution_count": 744,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threeD_X_train2.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1190</th>\n",
       "      <th>1191</th>\n",
       "      <th>1192</th>\n",
       "      <th>1193</th>\n",
       "      <th>1194</th>\n",
       "      <th>1195</th>\n",
       "      <th>1196</th>\n",
       "      <th>1197</th>\n",
       "      <th>1198</th>\n",
       "      <th>1199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>856.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>413.0</td>\n",
       "      <td>826.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>375.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>375.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>834.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>538.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>191.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>504.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>921.0</td>\n",
       "      <td>726.0</td>\n",
       "      <td>758.0</td>\n",
       "      <td>895.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>666.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>665.0</td>\n",
       "      <td>442.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>430.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>375.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>375.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>203.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>807.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>205 rows × 1200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2     3     4     5     6     7     8     9     ...   1190  \\\n",
       "0     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    8.0   \n",
       "1     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    8.0   \n",
       "2     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    8.0   \n",
       "3     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    3.0   \n",
       "4     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...  191.0   \n",
       "..    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...    ...   \n",
       "200   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    3.0   \n",
       "201   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    4.0   \n",
       "202   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    8.0   \n",
       "203   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    8.0   \n",
       "204   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...  203.0   \n",
       "\n",
       "      1191   1192   1193   1194   1195   1196   1197  1198   1199  \n",
       "0      5.0  856.0   19.0  413.0  826.0   17.0   22.0  23.0   40.0  \n",
       "1      5.0   60.0   10.0  375.0  350.0   17.0   22.0  23.0   40.0  \n",
       "2      5.0   60.0   10.0  375.0  350.0   17.0   22.0  23.0   40.0  \n",
       "3    834.0    6.0  101.0  188.0  538.0   17.0   22.0  23.0   40.0  \n",
       "4    224.0  504.0  111.0  921.0  726.0  758.0  895.0  45.0  666.0  \n",
       "..     ...    ...    ...    ...    ...    ...    ...   ...    ...  \n",
       "200    3.0    8.0    5.0  665.0  442.0   17.0   22.0  23.0   40.0  \n",
       "201   36.0  430.0    5.0   19.0  188.0   17.0   22.0  23.0   40.0  \n",
       "202    5.0   62.0   10.0  375.0  666.0   17.0   22.0  23.0   40.0  \n",
       "203    5.0   62.0   10.0  375.0  666.0   17.0   22.0  23.0   40.0  \n",
       "204  165.0    3.0    4.0  188.0  807.0   17.0   22.0  23.0   40.0  \n",
       "\n",
       "[205 rows x 1200 columns]"
      ]
     },
     "execution_count": 745,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'fillna'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-746-845f18e00f5e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mthreeD_X_train2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'fillna'"
     ]
    }
   ],
   "source": [
    "threeD_X_train2.fillna(value='', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 747,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(threeD_X_train2[4][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, Dropout, Bidirectional \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#model.add(Embedding(vocab_size, 100)) #모델에 입력크기를 고정된 크기고 제한\n",
    "#model.add(Dense(2, activation='softmax'))\n",
    "model.add(Bidirectional(LSTM(128, input_shape = (len(threeD_X_train2),1))))\n",
    "# model.add(LSTM(128, batch_input_shape = (270,1,1), stateful = True))#,stateful=True\n",
    "#,return_sequences=True, input_shape = (300,1) 입력형식, stateful=True 상태유지\n",
    "model.add(Dropout(0.2, input_shape=(270,1)))\n",
    "initializer = tf.keras.initializers.HeNormal() #가중치 초기화\n",
    "model.add(Dense(90, activation='relu',kernel_initializer=initializer))\n",
    "\n",
    "model.add(Dense(90, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "9/9 [==============================] - 12s 898ms/step - loss: 1.3922 - acc: 0.2881 - sparse_categorical_accuracy: 0.2881 - val_loss: 1.4251 - val_acc: 0.2439 - val_sparse_categorical_accuracy: 0.2439\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.24390, saving model to best_model.h5\n",
      "Epoch 2/1000\n",
      "9/9 [==============================] - 7s 793ms/step - loss: 1.3895 - acc: 0.2956 - sparse_categorical_accuracy: 0.2956 - val_loss: 1.4102 - val_acc: 0.2195 - val_sparse_categorical_accuracy: 0.2195\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.24390\n",
      "Epoch 3/1000\n",
      "9/9 [==============================] - 7s 768ms/step - loss: 1.3875 - acc: 0.2926 - sparse_categorical_accuracy: 0.2926 - val_loss: 1.4134 - val_acc: 0.2439 - val_sparse_categorical_accuracy: 0.2439\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.24390\n",
      "Epoch 4/1000\n",
      "9/9 [==============================] - 7s 788ms/step - loss: 1.3784 - acc: 0.3119 - sparse_categorical_accuracy: 0.3119 - val_loss: 1.4136 - val_acc: 0.2439 - val_sparse_categorical_accuracy: 0.2439\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.24390\n",
      "Epoch 5/1000\n",
      "9/9 [==============================] - 8s 926ms/step - loss: 1.3968 - acc: 0.2334 - sparse_categorical_accuracy: 0.2334 - val_loss: 1.4014 - val_acc: 0.2195 - val_sparse_categorical_accuracy: 0.2195\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.24390\n",
      "Epoch 6/1000\n",
      "9/9 [==============================] - 8s 870ms/step - loss: 1.3828 - acc: 0.2415 - sparse_categorical_accuracy: 0.2415 - val_loss: 1.3913 - val_acc: 0.2439 - val_sparse_categorical_accuracy: 0.2439\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.24390\n",
      "Epoch 7/1000\n",
      "9/9 [==============================] - 8s 839ms/step - loss: 1.3776 - acc: 0.3026 - sparse_categorical_accuracy: 0.3026 - val_loss: 1.4017 - val_acc: 0.2439 - val_sparse_categorical_accuracy: 0.2439\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.24390\n",
      "Epoch 8/1000\n",
      "9/9 [==============================] - 8s 897ms/step - loss: 1.3746 - acc: 0.2743 - sparse_categorical_accuracy: 0.2743 - val_loss: 1.3989 - val_acc: 0.2195 - val_sparse_categorical_accuracy: 0.2195\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.24390\n",
      "Epoch 9/1000\n",
      "9/9 [==============================] - 8s 885ms/step - loss: 1.3859 - acc: 0.2642 - sparse_categorical_accuracy: 0.2642 - val_loss: 1.3862 - val_acc: 0.2195 - val_sparse_categorical_accuracy: 0.2195\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.24390\n",
      "Epoch 10/1000\n",
      "9/9 [==============================] - 8s 878ms/step - loss: 1.3568 - acc: 0.2932 - sparse_categorical_accuracy: 0.2932 - val_loss: 1.3796 - val_acc: 0.2195 - val_sparse_categorical_accuracy: 0.2195\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.24390\n",
      "Epoch 11/1000\n",
      "9/9 [==============================] - 8s 848ms/step - loss: 1.3580 - acc: 0.3193 - sparse_categorical_accuracy: 0.3193 - val_loss: 1.4286 - val_acc: 0.1951 - val_sparse_categorical_accuracy: 0.1951\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.24390\n",
      "Epoch 12/1000\n",
      "9/9 [==============================] - 8s 870ms/step - loss: 1.3477 - acc: 0.3740 - sparse_categorical_accuracy: 0.3740 - val_loss: 1.4324 - val_acc: 0.1463 - val_sparse_categorical_accuracy: 0.1463\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.24390\n",
      "Epoch 13/1000\n",
      "9/9 [==============================] - 7s 791ms/step - loss: 1.3428 - acc: 0.3595 - sparse_categorical_accuracy: 0.3595 - val_loss: 1.4167 - val_acc: 0.1707 - val_sparse_categorical_accuracy: 0.1707\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.24390\n",
      "Epoch 14/1000\n",
      "9/9 [==============================] - 8s 893ms/step - loss: 1.3488 - acc: 0.2887 - sparse_categorical_accuracy: 0.2887 - val_loss: 1.4237 - val_acc: 0.2927 - val_sparse_categorical_accuracy: 0.2927\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.24390 to 0.29268, saving model to best_model.h5\n",
      "Epoch 15/1000\n",
      "9/9 [==============================] - 9s 967ms/step - loss: 1.3487 - acc: 0.3736 - sparse_categorical_accuracy: 0.3736 - val_loss: 1.4101 - val_acc: 0.1951 - val_sparse_categorical_accuracy: 0.1951\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.29268\n",
      "Epoch 16/1000\n",
      "9/9 [==============================] - 8s 917ms/step - loss: 1.3712 - acc: 0.3542 - sparse_categorical_accuracy: 0.3542 - val_loss: 1.4288 - val_acc: 0.1707 - val_sparse_categorical_accuracy: 0.1707\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.29268\n",
      "Epoch 17/1000\n",
      "9/9 [==============================] - 8s 893ms/step - loss: 1.3413 - acc: 0.3542 - sparse_categorical_accuracy: 0.3542 - val_loss: 1.4417 - val_acc: 0.1951 - val_sparse_categorical_accuracy: 0.1951\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.29268\n",
      "Epoch 18/1000\n",
      "9/9 [==============================] - 8s 926ms/step - loss: 1.3553 - acc: 0.3404 - sparse_categorical_accuracy: 0.3404 - val_loss: 1.4027 - val_acc: 0.1707 - val_sparse_categorical_accuracy: 0.1707\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.29268\n",
      "Epoch 19/1000\n",
      "9/9 [==============================] - 8s 865ms/step - loss: 1.3364 - acc: 0.3564 - sparse_categorical_accuracy: 0.3564 - val_loss: 1.4254 - val_acc: 0.2683 - val_sparse_categorical_accuracy: 0.2683\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.29268\n",
      "Epoch 20/1000\n",
      "3/9 [=========>....................] - ETA: 5s - loss: 1.3285 - acc: 0.3389 - sparse_categorical_accuracy: 0.3389"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "#patience loss가 갑소해도 20번은 더해라\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc','sparse_categorical_accuracy'])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc','sparse_categorical_accuracy'])\n",
    "#history = model.fit(X_train2, y_train, epochs=100, callbacks=[es, mc], batch_size=20, validation_split=0.2)\n",
    "history = model.fit(threeD_X_train2, y_train, epochs=1000, callbacks=[es, mc], batch_size=20, validation_split=0.2)\n",
    "#validation_split 전체데이터(train)중 얼마를 학습할 것이냐\n",
    "#batch_size 계산 후 가중치를 넘길 계산 단위"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1233 test_function  *\n        return step_function(self, iterator)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1224 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1217 run_step  **\n        outputs = model.test_step(data)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1183 test_step\n        y_pred = self(x, training=False)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:219 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\n    ValueError: Input 0 of layer sequential_81 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 1200)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-726-ed012347cb07>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mloaded_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best_model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n 테스트 정확도: %.4f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mloaded_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# 학습 결과 그래프 그리기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[0;32m   1387\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1388\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1389\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1390\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1391\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    869\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    872\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    723\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 725\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    726\u001b[0m             *args, **kwds))\n\u001b[0;32m    727\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2968\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2969\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2970\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2971\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3194\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3195\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3196\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3197\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3198\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 990\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    975\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 977\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    978\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1233 test_function  *\n        return step_function(self, iterator)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1224 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1217 run_step  **\n        outputs = model.test_step(data)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1183 test_step\n        y_pred = self(x, training=False)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:219 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\n    ValueError: Input 0 of layer sequential_81 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 1200)\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model('best_model.h5')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test2, y_test)[1]))\n",
    "\n",
    "# 학습 결과 그래프 그리기\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "\n",
    "acc_ax.plot(history.history['sparse_categorical_accuracy'], 'g', label='train acc')\n",
    "acc_ax.plot(history.history['val_sparse_categorical_accuracy'], 'b', label='val acc')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "plt.show()\n",
    "\n",
    "# # 끝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 상태유지 스택 순환신경망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_test\n"
     ]
    }
   ],
   "source": [
    "def train_test(data, per):\n",
    "    train, test = train_test_split(data, test_size= per, random_state=1234)\n",
    "\n",
    "    #인덱스 초기화\n",
    "    train = train.reset_index()\n",
    "    test = test.reset_index()\n",
    "    return train, test\n",
    "\n",
    "train, test = train_test(all_data,0.2)\n",
    "print(\"train_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(train['content'])\n",
    "X_test = tokenizer.texts_to_sequences(test['content'])\n",
    "\n",
    "y_train = train['per']\n",
    "y_train = pd.DataFrame(y_train)\n",
    "y_test = test['per']\n",
    "y_test = pd.DataFrame(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen = max_len)\n",
    "X_test = pad_sequences(X_test, maxlen = max_len)\n",
    "\n",
    "\n",
    "\n",
    "X_train2 = pd.DataFrame(X_train)\n",
    "X_train2=X_train2.astype('float64')\n",
    "X_test2 = pd.DataFrame(X_test)\n",
    "X_test2=X_test2.astype('float64')\n",
    "\n",
    "\n",
    "\n",
    "y_train=y_train.astype('float64')\n",
    "y_test=y_test.astype('float64')\n",
    "\n",
    "\n",
    "\n",
    "X_train2 = np.array(X_train2).reshape(X_train2.shape[0], X_train2.shape[1], 1)\n",
    "X_test2 = np.array(X_test2).reshape(X_test2.shape[0], X_test2.shape[1], 1)\n",
    "y_train = np.array(y_train).reshape(y_train.shape[0], y_train.shape[1], 1)\n",
    "y_test = np.array(y_test).reshape(y_test.shape[0], y_test.shape[1], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33, 800, 1)"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_train2=np.delete(X_train2, (0,1,2,3,4,5,6,7,8,9), axis = 0)\n",
    "X_test2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, batch_input_shape = (24,X_train2.shape[1], 1), stateful = True))#,stateful=True\n",
    "# model.add(LSTM(128, batch_input_shape = (X_train2.shape[1],X_train2.shape[0],1), stateful = True))\n",
    "# model.add(Dropout(0.2, input_shape=(270,1)))\n",
    "\n",
    "#initializer = tf.keras.initializers.HeNormal() #가중치 초기화\n",
    "# model.add(Dense(90, activation='relu',kernel_initializer=initializer))\n",
    "model.add(Dense(90, activation='relu'))\n",
    "model.add(Dense(90, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "4/4 [==============================] - 4s 697ms/step - loss: 1.3911 - acc: 0.1806 - categorical_accuracy: 0.5333 - val_loss: 1.2405 - val_acc: 0.3333 - val_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.33333, saving model to best_model.h5\n",
      "Epoch 2/1000\n",
      "4/4 [==============================] - 2s 385ms/step - loss: 1.2450 - acc: 0.3417 - categorical_accuracy: 0.5722 - val_loss: 1.1817 - val_acc: 0.3333 - val_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.33333\n",
      "Epoch 3/1000\n",
      "4/4 [==============================] - 2s 407ms/step - loss: 1.1931 - acc: 0.3069 - categorical_accuracy: 0.6986 - val_loss: 1.1654 - val_acc: 0.3333 - val_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.33333\n",
      "Epoch 4/1000\n",
      "4/4 [==============================] - 2s 390ms/step - loss: 1.1722 - acc: 0.4431 - categorical_accuracy: 1.0000 - val_loss: 1.1714 - val_acc: 0.3333 - val_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.33333\n",
      "Epoch 5/1000\n",
      "4/4 [==============================] - 1s 366ms/step - loss: 1.2238 - acc: 0.4014 - categorical_accuracy: 1.0000 - val_loss: 1.1602 - val_acc: 0.3750 - val_categorical_accuracy: 0.9583\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.33333 to 0.37500, saving model to best_model.h5\n",
      "Epoch 6/1000\n",
      "4/4 [==============================] - 2s 383ms/step - loss: 1.1375 - acc: 0.4431 - categorical_accuracy: 0.9028 - val_loss: 1.1574 - val_acc: 0.4167 - val_categorical_accuracy: 0.5833\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.37500 to 0.41667, saving model to best_model.h5\n",
      "Epoch 7/1000\n",
      "4/4 [==============================] - 1s 370ms/step - loss: 1.2166 - acc: 0.3403 - categorical_accuracy: 0.8208 - val_loss: 1.1675 - val_acc: 0.4167 - val_categorical_accuracy: 0.5417\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.41667\n",
      "Epoch 8/1000\n",
      "4/4 [==============================] - 1s 368ms/step - loss: 1.1436 - acc: 0.4792 - categorical_accuracy: 0.7472 - val_loss: 1.1761 - val_acc: 0.4167 - val_categorical_accuracy: 0.5833\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.41667\n",
      "Epoch 9/1000\n",
      "4/4 [==============================] - 2s 403ms/step - loss: 1.1123 - acc: 0.3764 - categorical_accuracy: 0.7097 - val_loss: 1.1770 - val_acc: 0.4167 - val_categorical_accuracy: 0.5833\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.41667\n",
      "Epoch 10/1000\n",
      "4/4 [==============================] - 2s 394ms/step - loss: 1.1935 - acc: 0.4375 - categorical_accuracy: 0.9167 - val_loss: 1.1964 - val_acc: 0.3333 - val_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.41667\n",
      "Epoch 11/1000\n",
      "4/4 [==============================] - 1s 376ms/step - loss: 1.1859 - acc: 0.4583 - categorical_accuracy: 1.0000 - val_loss: 1.2043 - val_acc: 0.3333 - val_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.41667\n",
      "Epoch 12/1000\n",
      "4/4 [==============================] - 2s 395ms/step - loss: 1.1930 - acc: 0.4153 - categorical_accuracy: 0.9292 - val_loss: 1.1834 - val_acc: 0.3750 - val_categorical_accuracy: 0.7917\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.41667\n",
      "Epoch 13/1000\n",
      "4/4 [==============================] - 1s 374ms/step - loss: 1.2051 - acc: 0.4208 - categorical_accuracy: 0.5875 - val_loss: 1.1835 - val_acc: 0.2917 - val_categorical_accuracy: 0.9583\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.41667\n",
      "Epoch 14/1000\n",
      "4/4 [==============================] - 1s 373ms/step - loss: 1.1994 - acc: 0.4708 - categorical_accuracy: 0.8264 - val_loss: 1.1765 - val_acc: 0.4167 - val_categorical_accuracy: 0.5833\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.41667\n",
      "Epoch 15/1000\n",
      "4/4 [==============================] - 1s 373ms/step - loss: 1.2178 - acc: 0.4139 - categorical_accuracy: 0.4986 - val_loss: 1.1786 - val_acc: 0.5000 - val_categorical_accuracy: 0.4167\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.41667 to 0.50000, saving model to best_model.h5\n",
      "Epoch 16/1000\n",
      "4/4 [==============================] - 2s 411ms/step - loss: 1.2007 - acc: 0.4472 - categorical_accuracy: 0.6917 - val_loss: 1.1967 - val_acc: 0.3333 - val_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.50000\n",
      "Epoch 17/1000\n",
      "4/4 [==============================] - 1s 370ms/step - loss: 1.1290 - acc: 0.4625 - categorical_accuracy: 0.9458 - val_loss: 1.1983 - val_acc: 0.2917 - val_categorical_accuracy: 0.9583\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.50000\n",
      "Epoch 18/1000\n",
      "4/4 [==============================] - 1s 370ms/step - loss: 1.1960 - acc: 0.4861 - categorical_accuracy: 0.9125 - val_loss: 1.1965 - val_acc: 0.2917 - val_categorical_accuracy: 0.9583\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.50000\n",
      "Epoch 19/1000\n",
      "4/4 [==============================] - 1s 386ms/step - loss: 1.1557 - acc: 0.4944 - categorical_accuracy: 0.8833 - val_loss: 1.1912 - val_acc: 0.3750 - val_categorical_accuracy: 0.8750\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.50000\n",
      "Epoch 20/1000\n",
      "4/4 [==============================] - 2s 392ms/step - loss: 1.1807 - acc: 0.4361 - categorical_accuracy: 0.7111 - val_loss: 1.1973 - val_acc: 0.2917 - val_categorical_accuracy: 0.9583\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.50000\n",
      "Epoch 21/1000\n",
      "4/4 [==============================] - 1s 377ms/step - loss: 1.1542 - acc: 0.5486 - categorical_accuracy: 0.8917 - val_loss: 1.2004 - val_acc: 0.2917 - val_categorical_accuracy: 0.9583\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.50000\n",
      "Epoch 22/1000\n",
      "4/4 [==============================] - 1s 370ms/step - loss: 1.1485 - acc: 0.4653 - categorical_accuracy: 0.7069 - val_loss: 1.1920 - val_acc: 0.4583 - val_categorical_accuracy: 0.7917\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.50000\n",
      "Epoch 23/1000\n",
      "4/4 [==============================] - 1s 374ms/step - loss: 1.1872 - acc: 0.3819 - categorical_accuracy: 0.3431 - val_loss: 1.1975 - val_acc: 0.5417 - val_categorical_accuracy: 0.4583\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.50000 to 0.54167, saving model to best_model.h5\n",
      "Epoch 24/1000\n",
      "4/4 [==============================] - 1s 374ms/step - loss: 1.1668 - acc: 0.4556 - categorical_accuracy: 0.6250 - val_loss: 1.2276 - val_acc: 0.3333 - val_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.54167\n",
      "Epoch 25/1000\n",
      "4/4 [==============================] - 2s 393ms/step - loss: 1.1319 - acc: 0.4444 - categorical_accuracy: 0.9653 - val_loss: 1.2032 - val_acc: 0.3333 - val_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.54167\n",
      "Epoch 26/1000\n",
      "4/4 [==============================] - 1s 374ms/step - loss: 1.1456 - acc: 0.4986 - categorical_accuracy: 0.9042 - val_loss: 1.1923 - val_acc: 0.4167 - val_categorical_accuracy: 0.5833\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.54167\n",
      "Epoch 00026: early stopping\n"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "#patience loss가 갑소해도 20번은 더해라\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc','categorical_accuracy'])\n",
    "#history = model.fit(X_train2, y_train, epochs=100, callbacks=[es, mc], batch_size=20, validation_split=0.2)\n",
    "history = model.fit(X_train2, y_train, epochs=1000, batch_size=24, validation_split=0.2,callbacks=[es, mc])\n",
    "#validation_split 전체데이터(train)중 얼마를 학습할 것이냐\n",
    "#batch_size 계산 후 가중치를 넘길 계산 단위"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "   Specified a list with shape [24,1] from a tensor with shape [32,1]\n\t [[{{node TensorArrayUnstack/TensorListFromTensor}}]]\n\t [[sequential_67/lstm_67/PartitionedCall]] [Op:__inference_test_function_203588]\n\nFunction call stack:\ntest_function -> test_function -> test_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-277-ed012347cb07>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mloaded_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best_model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n 테스트 정확도: %.4f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mloaded_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# 학습 결과 그래프 그리기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[0;32m   1387\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1388\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1389\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1390\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1391\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    892\u001b[0m               *args, **kwds)\n\u001b[0;32m    893\u001b[0m       \u001b[1;31m# If we did not create any variables the trace we have is good enough.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 894\u001b[1;33m       return self._concrete_stateful_fn._call_flat(\n\u001b[0m\u001b[0;32m    895\u001b[0m           filtered_flat_args, self._concrete_stateful_fn.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m:    Specified a list with shape [24,1] from a tensor with shape [32,1]\n\t [[{{node TensorArrayUnstack/TensorListFromTensor}}]]\n\t [[sequential_67/lstm_67/PartitionedCall]] [Op:__inference_test_function_203588]\n\nFunction call stack:\ntest_function -> test_function -> test_function\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model('best_model.h5')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test2, y_test)[1]))\n",
    "\n",
    "# 학습 결과 그래프 그리기\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "\n",
    "acc_ax.plot(history.history['sparse_categorical_accuracy'], 'g', label='train acc')\n",
    "acc_ax.plot(history.history['val_sparse_categorical_accuracy'], 'b', label='val acc')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "plt.show()\n",
    "\n",
    "# # 끝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텐서보드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "keyword argument repeated (<ipython-input-33-134858f07bc8>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-33-134858f07bc8>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    model.fit(X_train2, y_train, epochs=100, callbacks=[es, mc], batch_size=60, validation_split=0.2, callbacks = [tb_hist])\u001b[0m\n\u001b[1;37m                                                                                                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m keyword argument repeated\n"
     ]
    }
   ],
   "source": [
    "# tb_hist = keras.callbacks.TensorBoard(log_dir='.\\graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "# model.fit(X_train2, y_train, epochs=100, callbacks=[es, mc,tb_hist], batch_size=60, validation_split=0.2)\n",
    "# 순환신경망은 fit가 여러번 일어나 제대로 학습상태를 알 수 없다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#텐서보드 실행을 위해 콘솔에서 텐서보드를 띄운다\n",
    "#tensorboard --logdir=~/projects/Keras/_writing/graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Customhistory( keras.callbacks.Callback):\n",
    "    def init(self):\n",
    "        self.train_loss=[]\n",
    "        self.val_loss=[]\n",
    "        self.train_acc=[]\n",
    "        self.val_acc=[]\n",
    "        \n",
    "    def on_epoch_end(self, batch, log={}):\n",
    "        self.train_loss.append(logs.get('loss'))\n",
    "        self.val_loss.append(logs.get('val_loss'))\n",
    "        self.train_acc.append(logs.get('acc'))\n",
    "        self.val_acc.append(logs.get('val_acc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customhist = Customhistory()\n",
    "for epoch_idx in range(150):\n",
    "    print('epochs : ' + str(epoch_idx))\n",
    "    model.fit(X_train2, y_train, callbacks=[mc,tb_hist], batch_size=60, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
